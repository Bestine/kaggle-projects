{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":531628,"sourceType":"datasetVersion","datasetId":252977}],"dockerImageVersionId":29297,"isInternetEnabled":false,"language":"rmarkdown","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n---\ntitle: \"**Market Basket Analysis**\"\nauthor: \"Bestine Okinda\"\ndate: '`r Sys.Date()`'\noutput:\n  html_document:\n    toc: yes\n    number_sections: yes\n    code_folding: hide\n    theme: cosmo\n    highlight: tango\n---\n\n<center><img\nsrc=\"https://i.imgur.com/Opyn1vo.png\">\n</center>\n\n# **Introduction**\n\nHi! In this kernel we are going to use the **Apriori algorithm** to perform a **Market Basket Analysis**. A Market what? Is a technique used by large retailers to uncover associations between items.\nIt works by looking for combinations of items that occur together frequently in transactions, providing information to understand the purchase behavior. The outcome of this type of technique is, \nin simple terms, a set of **rules** that can be understood as **“if this, then that”**. For more information about these topics, please check in the following links: \n\n* [Market Basket Analysis](https://en.wikipedia.org/wiki/Affinity_analysis)\n\n* [Apriori algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm)\n\n* [Association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning)\n\nFirst it's important to define the Apriori algorithm, including some statistical concepts  (support, confidence, lift and conviction) to select interesting rules. \nThen we are going to use a data set containing more than 6.000 transactions from a bakery to apply the algorithm and find combinations of products that are bought together. Let's start!\n\n# **Association rules**\n \nThe Apriori algorithm generates association rules for a given data set. An association rule implies that if an item A occurs, then item B also occurs with a certain probability. \nLet's see an example, \n\n<center>\n\n| Transaction   | Items                                       | \n|:--------------|:--------------------------------------------|\n| t1            | {T-shirt, Trousers, Belt}                   | \n| t2            | {T-shirt, Jacket}                           |   \n| t3            | {Jacket, Gloves}                            |  \n| t4            | {T-shirt, Trousers, Jacket}                 | \n| t5            | {T-shirt, Trousers, Sneakers, Jacket, Belt} |   \n| t6            | {Trousers, Sneakers, Belt}                  |\n| t7            | {Trousers, Belt, Sneakers}                  | \n\n</center>   \n\nIn the table above we can see seven transactions from a clothing store. Each transaction shows items bought in that transaction. We can represent our items as an **item set** as follows:\n\n$$I=\\{i_1, i_2,..., i_k\\}$$\n\nIn our case it corresponds to:\n\n$$I=\\{T\\text- shirt, Trousers, Belt, Jacket, Gloves, Sneakers\\}$$\n\nA **transaction** is represented by the following expression:\n\n$$T=\\{t_1, t_2,..., t_n\\}$$\n\nFor example,\n\n$$t_1=\\{T\\text- shirt, Trousers, Belt\\}$$\n\nThen, an **association rule** is defined as an implication of the form:\n\n<center> $X \\Rightarrow Y$, where $X \\subset I$, $Y \\subset I$ and $X \\cap Y = 0$ </center>\n\nFor example, \n\n$$\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\}$$\n\nIn the following sections we are going to define four metrics to measure the precision of a rule. \n\n## Support\n\nSupport is an indication of how frequently the item set appears in the data set.\n\n$$supp(X \\Rightarrow Y)=\\dfrac{|X \\cup Y|}{n}$$\n\nIn other words, it's the number of transactions with both $X$ and $Y$ divided by the total number of transactions. The rules are not useful for low support values. \nLet's see different examples using the clothing store transactions from the previous table. \n\n* $supp(T\\text- shirt \\Rightarrow Trousers)=\\dfrac{3}{7}=43 \\%$\n\n* $supp(Trousers \\Rightarrow Belt)=\\dfrac{4}{7}= 57 \\%$\n\n* $supp(T\\text- shirt \\Rightarrow Belt)=\\dfrac{2}{7}=28 \\%$\n\n* $supp(\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\})=\\dfrac{2}{7}=28 \\%$\n\n## Confidence\n\nFor a rule $X \\Rightarrow Y$, confidence shows the percentage in which $Y$ is bought with $X$. It's an indication of how often the rule has been found to be true.\n\n$$conf(X \\Rightarrow Y)=\\dfrac{supp(X \\cup Y)}{supp(X)}$$\n\nFor example, the rule $T\\text- shirt \\Rightarrow Trousers$ has a confidence of 3/4, which means that for 75% of the transactions containing a t-shirt the rule is correct \n(75% of the times a customer buys a t-shirt, trousers are bought as well). Three more examples:\n\n* $conf(Trousers \\Rightarrow Belt)=\\dfrac{4/7}{5/7}= 80 \\%$\n\n* $conf(T\\text- shirt \\Rightarrow Belt)=\\dfrac{2/7}{4/7}=50 \\%$\n\n* $conf(\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\})=\\dfrac{2/7}{3/7}=66 \\%$\n\n## Lift\n\nThe lift of a rule is the ratio of the observed support to that expected if $X$ and $Y$ were independent, and is defined as\n\n$$lift(X \\Rightarrow Y)=\\dfrac{supp(X \\cup Y)}{supp(X)supp(Y) }$$\n\nGreater lift values indicate stronger associations. Let's see some examples:\n\n* $lift(T\\text- shirt \\Rightarrow Trousers)=\\dfrac{3/7}{(4/7)(5/7)}= 1.05$\n\n* $lift(Trousers \\Rightarrow Belt)=\\dfrac{4/7}{(5/7)(4/7)}= 1.4$\n\n* $lift(T\\text- shirt \\Rightarrow Belt)=\\dfrac{2/7}{(4/7)(4/7)}=0.875$\n\n* $lift(\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\})=\\dfrac{2/7}{(3/7)(4/7)}=1.17$\n\n## Conviction\n\nThe conviction of a rule is defined as \n\n$$conv(X \\Rightarrow Y)=\\dfrac{1-supp(Y)}{1-conf(X \\Rightarrow Y) }$$\n\nIt can be interpreted as the ratio of the expected frequency that $X$ occurs without $Y$ if $X$  and $Y$ were independent divided by the observed frequency of incorrect predictions. \nA high value means that the consequent depends strongly on the antecedent. Let's see some examples: \n\n* $conv(T\\text- shirt \\Rightarrow Trousers)= \\dfrac{1-5/7}{1-3/4}=1.14$\n\n* $conv(Trousers \\Rightarrow Belt)= \\dfrac{1-4/7}{1-4/5}=2.14$\n\n* $conv(T\\text- shirt \\Rightarrow Belt)=\\dfrac{1-4/7}{1-1/2}=0.86$\n\n* $conv(\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\})=\\dfrac{1-4/7}{1-2/3}=1.28$\n\nIf you want more information about these measures, please check [here](https://en.wikipedia.org/wiki/Association_rule_learning).\n\n# **Loading Data** {.tabset .tabset-fade .tabset-pills}\n\nFirst we need to load some libraries and import our data. We can use the function [`read.transactions()`](https://www.rdocumentation.org/packages/arules/versions/1.6-5/topics/read.transactions) \nfrom the [`arules`](https://cran.r-project.org/web/packages/arules/arules.pdf) package to create a `transactions` object. \n\n```{r message=FALSE, warning=FALSE}\n# Load libraries\nlibrary(tidyverse) # data manipulation\nlibrary(arules) # mining association rules and frequent itemsets\nlibrary(arulesViz) # visualization techniques for association rules\nlibrary(knitr) # dynamic report generation\nlibrary(gridExtra) # provides a number of user-level functions to work with \"grid\" graphics\nlibrary(lubridate) # work with dates and times\n\n# Read the data\ntrans <- read.transactions(\"../input/BreadBasket_DMS.csv\", format=\"single\", cols=c(3,4), sep=\",\", rm.duplicates=TRUE)\n```\n\nLet’s get an idea of what we’re working with.\n\n## Transaction object\n```{r message=FALSE, warning=FALSE}\n# Transaction object\ntrans\n```\n\n## Summary\n```{r message=FALSE, warning=FALSE}\n# Summary\nsummary(trans)\n```\n\n## Glimpse\n```{r message=FALSE, warning=FALSE}\n# Glimpse\nglimpse(trans)\n```\n\n## Structure\n```{r message=FALSE, warning=FALSE}\n# Structure\nstr(trans)\n```\n\n# **Data Dictionary** \n\nThe data set contains 15.010 observations and the following columns,\n\n* `Date`. Categorical variable that tells us the date of the transactions (YYYY-MM-DD format). The column includes dates from 30/10/2016 to 09/04/2017.\n\n* `Time`. Categorical variable that tells us the time of the transactions (HH:MM:SS format).\n\n* `Transaction`. Quantitative variable that allows us to differentiate the transactions. The rows that share the same value in this field belong to the same transaction, that's why the data set has less transactions than observations. \n\n* `Item`. Categorical variable containing the products. \n\n# **Data Analysis**\n\nBefore applying the Apriori algorithm on the data set, we are going to show some visualizations to learn more about the transactions. For example, we can use the `itemFrequencyPlot()` function\nto create an item frequency bar plot, in order to view the distribution of products. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Absolute Item Frequency Plot\nitemFrequencyPlot(trans, topN=15, type=\"absolute\", col=\"wheat2\",xlab=\"Item name\", \n                  ylab=\"Frequency (absolute)\", main=\"Absolute Item Frequency Plot\")\n```\n \nThe [`itemFrequencyPlot()`](https://www.rdocumentation.org/packages/arules/versions/1.6-5/topics/itemFrequencyPlot) allows us to show the absolute or relative values.\nIf absolute it will plot numeric frequencies of each item independently. If relative it will plot how many times these items have appeared as compared to others,\nas it's shown in the following plot. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Relative Item Frequency Plot\nitemFrequencyPlot(trans, topN=15, type=\"relative\", col=\"lightcyan2\", xlab=\"Item name\", \n                  ylab=\"Frequency (relative)\", main=\"Relative Item Frequency Plot\")\n```\n   \nCoffee is the best-selling product by far, followed by bread and tea. Let's display some other visualizations describing the time distribution using the \n[`ggplot()`](https://www.rdocumentation.org/packages/ggplot2/versions/3.3.0/topics/ggplot) function. \n\n* Transactions per month\n\n* Transactions per weekday\n\n* Transactions per hour \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Load data \ntrans_csv <- read.csv(\"../input/BreadBasket_DMS.csv\")\n\n# Visualization - Transactions per month\ntrans_csv %>%\n  mutate(Month=as.factor(month(Date))) %>%\n  group_by(Month) %>%\n  summarise(Transactions=n_distinct(Transaction)) %>%\n  ggplot(aes(x=Month, y=Transactions)) +\n  geom_bar(stat=\"identity\", fill=\"mistyrose2\", \n           show.legend=FALSE, colour=\"black\") +\n  geom_label(aes(label=Transactions)) +\n  labs(title=\"Transactions per month\") +\n  theme_bw() \n```\n\nThe data set includes dates from 30/10/2016 to 09/04/2017, that's why we have so few transactions in October and April. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Visualization - Transactions per weekday\ntrans_csv %>%\n  mutate(WeekDay=as.factor(weekdays(as.Date(Date)))) %>%\n  group_by(WeekDay) %>%\n  summarise(Transactions=n_distinct(Transaction)) %>%\n  ggplot(aes(x=WeekDay, y=Transactions)) +\n  geom_bar(stat=\"identity\", fill=\"peachpuff2\", \n           show.legend=FALSE, colour=\"black\") +\n  geom_label(aes(label=Transactions)) +\n  labs(title=\"Transactions per weekday\") +\n  scale_x_discrete(limits=c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\",\n                            \"Friday\", \"Saturday\", \"Sunday\")) +\n  theme_bw() \n```\n\nAs we can see, Saturday is the busiest day in the bakery. Conversely, Wednesday is the day with fewer transactions.\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Visualization - Transactions per hour\ntrans_csv %>%\n  mutate(Hour=as.factor(hour(hms(Time)))) %>%\n  group_by(Hour) %>%\n  summarise(Transactions=n_distinct(Transaction)) %>%\n  ggplot(aes(x=Hour, y=Transactions)) +\n  geom_bar(stat=\"identity\", fill=\"steelblue1\", show.legend=FALSE, colour=\"black\") +\n  geom_label(aes(label=Transactions)) +\n  labs(title=\"Transactions per hour\") +\n  theme_bw()\n```\n\nThere’s not much to discuss with this visualization. The results are logical and expected.\n\n# **Apriori algorithm**\n\n## Choice of support and confidence\n\nThe first step in order to create a set of association rules is to determine the optimal thresholds for support and confidence.  If we set these values too low, then the algorithm will take \nlonger to execute and we will get a lot of rules (most of them will not be useful). Then, what values do we choose? We can try different values of support and confidence\nand see graphically how many rules are generated for each combination.\n\n```{r results='hide', message=FALSE, warning=FALSE}\n# Support and confidence values\nsupportLevels <- c(0.1, 0.05, 0.01, 0.005)\nconfidenceLevels <- c(0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1)\n\n# Empty integers \nrules_sup10 <- integer(length=9)\nrules_sup5 <- integer(length=9)\nrules_sup1 <- integer(length=9)\nrules_sup0.5 <- integer(length=9)\n\n# Apriori algorithm with a support level of 10%\nfor (i in 1:length(confidenceLevels)) {\n  \n  rules_sup10[i] <- length(apriori(trans, parameter=list(sup=supportLevels[1], \n                                   conf=confidenceLevels[i], target=\"rules\")))\n  \n}\n\n# Apriori algorithm with a support level of 5%\nfor (i in 1:length(confidenceLevels)){\n  \n  rules_sup5[i] <- length(apriori(trans, parameter=list(sup=supportLevels[2], \n                                  conf=confidenceLevels[i], target=\"rules\")))\n  \n}\n\n# Apriori algorithm with a support level of 1%\nfor (i in 1:length(confidenceLevels)){\n  \n  rules_sup1[i] <- length(apriori(trans, parameter=list(sup=supportLevels[3], \n                                  conf=confidenceLevels[i], target=\"rules\")))\n  \n}\n\n# Apriori algorithm with a support level of 0.5%\nfor (i in 1:length(confidenceLevels)){\n  \n  rules_sup0.5[i] <- length(apriori(trans, parameter=list(sup=supportLevels[4], \n                                    conf=confidenceLevels[i], target=\"rules\")))\n  \n}\n```\n\nIn the following graphs we can see the number of rules generated with a support level of 10%, 5%, 1% and 0.5%. \n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Number of rules found with a support level of 10%\nplot1 <- qplot(confidenceLevels, rules_sup10, geom=c(\"point\", \"line\"), \n               xlab=\"Confidence level\", ylab=\"Number of rules found\", \n               main=\"Apriori with a support level of 10%\") +\n  theme_bw()\n\n# Number of rules found with a support level of 5%\nplot2 <- qplot(confidenceLevels, rules_sup5, geom=c(\"point\", \"line\"), \n               xlab=\"Confidence level\", ylab=\"Number of rules found\", \n               main=\"Apriori with a support level of 5%\") + \n  scale_y_continuous(breaks=seq(0, 10, 2)) +\n  theme_bw()\n\n# Number of rules found with a support level of 1%\nplot3 <- qplot(confidenceLevels, rules_sup1, geom=c(\"point\", \"line\"), \n               xlab=\"Confidence level\", ylab=\"Number of rules found\", \n               main=\"Apriori with a support level of 1%\") + \n  scale_y_continuous(breaks=seq(0, 50, 10)) +\n  theme_bw()\n\n# Number of rules found with a support level of 0.5%\nplot4 <- qplot(confidenceLevels, rules_sup0.5, geom=c(\"point\", \"line\"), \n               xlab=\"Confidence level\", ylab=\"Number of rules found\", \n               main=\"Apriori with a support level of 0.5%\") + \n  scale_y_continuous(breaks=seq(0, 130, 20)) +\n  theme_bw()\n\n# Subplot\ngrid.arrange(plot1, plot2, plot3, plot4, ncol=2)\n```\n  \nWe can join the four lines to improve the visualization.\n\n```{r message=FALSE, warning=FALSE, fig.align='center'}\n# Data frame\nnum_rules <- data.frame(rules_sup10, rules_sup5, rules_sup1, rules_sup0.5, confidenceLevels)\n\n# Number of rules found with a support level of 10%, 5%, 1% and 0.5%\nggplot(data=num_rules, aes(x=confidenceLevels)) +\n  \n  # Plot line and points (support level of 10%)\n  geom_line(aes(y=rules_sup10, colour=\"Support level of 10%\")) + \n  geom_point(aes(y=rules_sup10, colour=\"Support level of 10%\")) +\n  \n  # Plot line and points (support level of 5%)\n  geom_line(aes(y=rules_sup5, colour=\"Support level of 5%\")) +\n  geom_point(aes(y=rules_sup5, colour=\"Support level of 5%\")) +\n  \n  # Plot line and points (support level of 1%)\n  geom_line(aes(y=rules_sup1, colour=\"Support level of 1%\")) + \n  geom_point(aes(y=rules_sup1, colour=\"Support level of 1%\")) +\n  \n  # Plot line and points (support level of 0.5%)\n  geom_line(aes(y=rules_sup0.5, colour=\"Support level of 0.5%\")) +\n  geom_point(aes(y=rules_sup0.5, colour=\"Support level of 0.5%\")) +\n  \n  # Labs and theme\n  labs(x=\"Confidence levels\", y=\"Number of rules found\", \n       title=\"Apriori algorithm with different support levels\") +\n  theme_bw() +\n  theme(legend.title=element_blank())\n```\n  \nLet's analyze the results,\n\n* **Support level of 10%**. We only identify a few rules with very low confidence levels. This means that there are no relatively frequent associations in our data set. We can't choose this value, the resulting rules are unrepresentative.\n\n* **Support level of 5%**. We only identify a rule with a confidence of at least 50%. It seems that we have to look for support levels below 5% to obtain a greater number of rules with a reasonable confidence. \n\n* **Support level of 1%**. We started to get dozens of rules, of which 13 have a confidence of at least 50%.\n\n* **Support level of 0.5%**. Too many rules to analyze!\n\nTo sum up, we are going to use a support level of 1% and a confidence level of 50%. \n\n## Execution\n\nLet's execute the Apriori algorithm with the values obtained in the previous section.\n\n```{r results='hide', message=FALSE, warning=FALSE}\n# Apriori algorithm execution with a support level of 1% and a confidence level of 50%\nrules_sup1_conf50 <- apriori(trans, parameter=list(sup=supportLevels[3], \n                             conf=confidenceLevels[5], target=\"rules\"))\n```\n\nThe generated association rules are the following,\n\n```{r message=FALSE, warning=FALSE}\n# Inspect association rules\ninspect(rules_sup1_conf50)\n```\n\nWe can also create an HTML table widget using the [`inspectDT()`](https://www.rdocumentation.org/packages/arulesViz/versions/1.3-3/topics/inspectDT) function from the `aruslesViz` package.\nRules can be interactively filtered and sorted.\n\nHow do we interpret these rules? \n\n* 52% of the customers who bought a hot chocolate algo bought a coffee.\n\n* 63% of the customers who bought a spanish brunch also bought a coffee.\n\n* 73% of the customers who bought a toast also bought a coffee.\n\nAnd so on. It seems that in this bakery there are many coffee lovers!\n\n## Visualize association rules\n\nWe are going to use the `arulesViz` package to create the visualizations. Let's begin with a simple scatter plot with different measures of interestingness on the axes (lift and support) \nand a third measure (confidence) represented by the color of the points. \n\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Scatter plot\nplot(rules_sup1_conf50, measure=c(\"support\", \"lift\"), shading=\"confidence\")\n```\n \nThe following visualization represents the rules as a graph with items as labeled vertices, and rules represented as vertices connected to items using arrows.\n\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Graph (default layout)\nplot(rules_sup1_conf50, method=\"graph\")\n```\n \nWe can also change the graph layout.  \n\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Graph (circular layout)\nplot(rules_sup1_conf50, method=\"graph\", control=list(layout=igraph::in_circle()))\n```\n\nWhat else can we do? We can represent the rules as a grouped matrix-based visualization. The support and lift measures are represented by the size and color of the ballons, respectively. \nIn this case it's not a very useful visualization, since we only have coffe on the right-hand-side of the rules. \n\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Grouped matrix plot\nplot(rules_sup1_conf50, method=\"grouped\")\n```\n \nThere's an awesome function called [`ruleExplorer()`](https://www.rdocumentation.org/packages/arulesViz/versions/1.3-3/topics/ruleExplorer) that explores \nassociation rules using interactive manipulations and visualization using shiny. Unfortunately, R Markdown still doesn't support shiny app objects. \n\n## Another execution {.tabset .tabset-fade .tabset-pills}\n\nWe have executed the Apriori algorithm with the appropriate support and confidence values. What happens if we execute it with low values? How do the visualizations change? \nLet's try with a support level of 0.5% and a confidence level of 10%.\n\n```{r results='hide', message=FALSE, warning=FALSE}\n# Apriori algorithm execution with a support level of 0.5% and a confidence level of 10%\nrules_sup0.5_conf10 <- apriori(trans, parameter=list(sup=supportLevels[4], conf=confidenceLevels[9], target=\"rules\"))\n```\n\nIt's impossible to analyze these visualizations! For larger rule sets visual analysis becomes difficult. Furthermore, most of the rules are useless. \nThat's why we have to carefully select the right values of support and confidence. \n\n### Graph\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Graph (circular layout)\nplot(rules_sup0.5_conf10, method=\"graph\", control=list(layout=igraph::in_circle()))\n```\n  \n### Parallel coordinates plot\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Parallel coordinates plot\nplot(rules_sup0.5_conf10, method=\"paracoord\", control=list(reorder=TRUE))\n```\n  \n### Grouped matrix plot\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Grouped matrix plot\nplot(rules_sup0.5_conf10, method=\"grouped\")\n```\n\n### Scatter plot\n```{r fig.align='center', message=FALSE, warning=FALSE}\n# Scatter plot\nplot(rules_sup0.5_conf10, measure=c(\"support\", \"lift\"), shading=\"confidence\", jitter=0)\n```\n\n# **Exercises**\n\nIn this section you can test the concepts learned during this kernel by answering the following questionnaire. Good luck!\n\n* Give an example where you can apply the Apriori algorithm. \n\n* Calculate the support of the rule $Trousers \\Rightarrow Jacket$ using the clothing store transactions. Interpret the result.\n\n* Calculate the confidence of the rule $T\\text- shirt \\Rightarrow Jacket$ using the clothing store transactions. Interpret the result.\n\n* Calculate the lift of the rule $Trousers \\Rightarrow Sneakers$ using the clothing store transactions. Interpret the result. \n\n* Calculate the conviction of the rule $T\\text- shirt \\Rightarrow Belt$ using the clothing store transactions. Interpret the result.\n\n* Calculate the four metrics (support, confidence, lift and conviction) of the rule  $\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Jacket\\}$ using the clothing store transactions. Interpret the results.\n\n* What happens when we decrease the support level? Why?\n\n* What happens when we increase the conficence level? Why?\n\n* How many rules are generated with a support level of 0.5% and a confidence level of 20%? (you can use the previous visualizations)\n\n* Using the previous data set, execute the Apriori algorithm with a support level of 5% and a confidence level of 10%. Are the rules interesting? Why?\n\n* Prove the functions [`ruleExplorer()`](https://www.rdocumentation.org/packages/arulesViz/versions/1.3-3/topics/ruleExplorer) and \n[`inspectDT()`](https://www.rdocumentation.org/packages/arulesViz/versions/1.3-3/topics/inspectDT) from the package `arulesViz` on your RStudio environment. \n\n* What recommendations would you give to the owner of the bakery?\n\nHave you passed the test?\n\n# **Summary** \n\nIn this kernel we have learned about the Apriori algorithm, one of the most frequently used algorithms in data mining. We have reviewed some statistical concepts (support, confidence, \nlift and conviction) to select interesting rules, we have chosen the appropriate values to execute the algorithm and finally we have visualized the resulting association rules.\n\nAnd that’s it! It has been a pleasure to make this kernel, I have learned a lot! Thank you for reading and if you like it, please upvote it!\n    \nBy the way, if you want to view more kernels about other machine learning algorithms or statistical techniques, you can check the following links:\n\n* [Image Compression using PCA](https://www.kaggle.com/xvivancos/image-compression-using-pca)\n\n* [k-Nearest Neighbors algorithm (k-NN) in the Iris data set](https://www.kaggle.com/xvivancos/knn-in-the-iris-data-set)\n\n* [Clustering wines with k-means](https://www.kaggle.com/xvivancos/tutorial-clustering-wines-with-k-means)\n\n# **Citations for used packages**\n\nHadley Wickham (2017). tidyverse: Easily Install and Load the 'Tidyverse'. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\n\nMichael Hahsler, Christian Buchta, Bettina Gruen and Kurt Hornik (2018). arules: Mining Association Rules and Frequent Itemsets. R package version 1.6-1. https://CRAN.R-project.org/package=arules\n\nMichael Hahsler, Bettina Gruen and Kurt Hornik (2005), arules - A Computational Environment for Mining Association Rules and Frequent Item Sets.  Journal of Statistical Software 14/15.  URL: http://dx.doi.org/10.18637/jss.v014.i15.\n\nMichael Hahsler, Sudheer Chelluboina, Kurt Hornik, and Christian Buchta (2011), The arules R-package ecosystem: Analyzing interesting\npatterns from large transaction datasets.  Journal of Machine Learning Research, 12:1977--1981.  URL: http://jmlr.csail.mit.edu/papers/v12/hahsler11a.html.\n  \nMichael Hahsler (2018). arulesViz: Visualizing Association Rules and Frequent Itemsets. R package version 1.3-1. https://CRAN.R-project.org/package=arulesViz\n\nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.20.\n\nYihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595\n\nBaptiste Auguie (2017). gridExtra: Miscellaneous Functions for \"Grid\" Graphics. R package version 2.3. https://CRAN.R-project.org/package=gridExtra  ","metadata":{"_uuid":"274e3a30-8be3-44bb-8ace-a732f2577618","_cell_guid":"c31ab7eb-ee78-431e-a489-ba2fc3140c49","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}