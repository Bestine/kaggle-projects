{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":111543,"databundleVersionId":13750964,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =================================================================\n# 1. IMPORTS\n# =================================================================\nimport os\nfrom pathlib import Path\nimport datetime\nimport warnings\n\nfrom tqdm import tqdm\nfrom dataclasses import dataclass, asdict\n\nimport polars as pl\nimport numpy as np\nimport pandas as pd\n\n# Model Imports\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\nfrom sklearn.base import clone\nfrom scipy.stats import uniform, randint\n\n# Kaggle API\nimport kaggle_evaluation.default_inference_server\n\n# Silence warnings\nwarnings.filterwarnings('ignore')\nos.environ['PYTHONWARNINGS'] = 'ignore'\n\n\n# =================================================================\n# 2. CONFIGURATIONS\n# =================================================================\n\n# ============ PATHS ============\nDATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n\n# ============ RETURNS TO SIGNAL CONFIGS ============\nMIN_INVESTMENT: float = 0.0\nMAX_INVESTMENT: float = 2.0\nSIGNAL_MULTIPLIER: float = 400.0\n\n# ============ MODEL CONFIGS ============\nN_SPLITS_TSCV: int = 5      # Folds for time-series validation\nN_ITER_SEARCH: int = 100    # Number of tuning iterations\n\n# ============ DATACLASSES ============\n@dataclass\nclass DatasetOutput:\n    X_train : pl.DataFrame\n    X_test: pl.DataFrame\n    y_train: pl.Series\n    y_test: pl.Series\n    scaler: StandardScaler\n\n@dataclass(frozen=True)\nclass RetToSignalParameters:\n    signal_multiplier: float\n    min_signal : float = MIN_INVESTMENT\n    max_signal : float = MAX_INVESTMENT\n\nret_signal_params = RetToSignalParameters(\n    signal_multiplier= SIGNAL_MULTIPLIER\n)\n\n# Define the custom error for the score function\nclass ParticipantVisibleError(Exception):\n    pass\n\n# =================================================================\n# 3. PREPROCESSING & FEATURE ENGINEERING\n# =================================================================\n\ndef load_trainset() -> pl.DataFrame:\n    \"\"\"Loads and preprocesses the training dataset.\"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"train.csv\")\n        .rename({'market_forward_excess_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n        .head(-10) # Drop last 10 as in original\n    )\n\ndef load_testset() -> pl.DataFrame:\n    \"\"\"Loads and preprocesses the testing dataset.\"\"\"\n    return (\n        pl.read_csv(DATA_PATH / \"test.csv\")\n        .rename({'lagged_forward_returns':'target'})\n        .with_columns(\n            pl.exclude('date_id').cast(pl.Float64, strict=False)\n        )\n    )\n\ndef create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Creates new ROW-WISE features and cleans a DataFrame.\n    (Rolling features are now in a separate function)\n    \"\"\"\n    # Start with all original numeric features\n    original_features = [col for col in df.columns if col not in [\"date_id\", \"target\"]]\n    \n    # Define expressions for new features\n    # Use 1e-6 to avoid division by zero\n    feature_expressions = [\n        # Original features\n        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n        (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3 + 1e-6)).alias(\"U2\"),\n        \n        # Volatility ratios\n        (pl.col(\"V1\") / (pl.col(\"V2\") + 1e-6)).alias(\"V1_V2_ratio\"),\n        (pl.col(\"V1\") / (pl.col(\"I1\") + 1e-6)).alias(\"V1_I1_ratio\"),\n        \n        # Slope/Difference features\n        (pl.col(\"S1\") - pl.col(\"S5\")).alias(\"S1_S5_diff\"),\n        (pl.col(\"P8\") - pl.col(\"P9\")).alias(\"P8_P9_diff\"),\n        (pl.col(\"P12\") - pl.col(\"P13\")).alias(\"P12_P13_diff\"),\n        (pl.col(\"M1\") - pl.col(\"M2\")).alias(\"M1_M2_diff\"),\n        (pl.col(\"I2\") - pl.col(\"I9\")).alias(\"I2_I9_diff\"),\n        \n        # Ratio features\n        (pl.col(\"P8\") / (pl.col(\"P9\") + 1e-6)).alias(\"P8_P9_ratio\"),\n        (pl.col(\"E2\") / (pl.col(\"E3\") + 1e-6)).alias(\"E2_E3_ratio\"),\n        (pl.col(\"S2\") / (pl.col(\"S1\") + 1e-6)).alias(\"S2_S1_ratio\"),\n        \n        # Interaction features\n        (pl.col(\"E2\") * pl.col(\"E3\")).alias(\"E2_E3_prod\"),\n        (pl.col(\"P1\") * pl.col(\"V1\")).alias(\"P1_V1_prod\"),\n        ((pl.col(\"S1\") + pl.col(\"S5\")) / 2).alias(\"S1_S5_mean\"),\n        ((pl.col(\"P12\") + pl.col(\"P13\")) / (pl.col(\"V2\") + 1e-6)).alias(\"P12_P13_div_V2\"),\n    ]\n    \n    # Get names of new features\n    new_feature_names = [expr.meta.output_name() for expr in feature_expressions]\n    all_features = original_features + new_feature_names\n    \n    # Select all raw features + new row-wise features\n    # We keep all original features for the rolling calculations\n    select_cols = [\"date_id\", \"target\"] + all_features\n    \n    return (\n        df.with_columns(feature_expressions)\n        .select(select_cols) # Ensure we only have features we've defined\n        .with_columns([\n            # Use median for robust imputation. \n            pl.col(col).fill_null(pl.col(col).median())\n            for col in all_features\n        ])\n        .drop_nulls() # Drop any rows that are still null\n    )\n\n# --- NEW FUNCTION ---\ndef add_rolling_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Applies rolling (time-series) features to a DataFrame.\n    This function assumes df is already sorted by date_id.\n    \"\"\"\n    # EWM (Exponentially Weighted Moving) is good for time-series\n    # It reacts faster than simple rolling_mean and has no \"NaN\" gap\n    rolling_expressions = [\n        pl.col(\"M1\").ewm_mean(com=5).alias(\"M1_ewm_5\"),  # Short-term trend\n        pl.col(\"M1\").ewm_mean(com=21).alias(\"M1_ewm_21\"), # Long-term trend\n        (pl.col(\"M1\") - pl.col(\"M1\").ewm_mean(com=21)).alias(\"M1_momentum\"), # Momentum\n        \n        pl.col(\"V1\").ewm_mean(com=21).alias(\"V1_ewm_21\"), # Long-term volatility\n        (pl.col(\"V1\") - pl.col(\"V1\").ewm_mean(com=21)).alias(\"V1_vol_momentum\"),\n        \n        pl.col(\"S1\").ewm_mean(com=10).alias(\"S1_ewm_10\"),\n        pl.col(\"P8\").ewm_mean(com=10).alias(\"P8_ewm_10\"),\n    ]\n    \n    return df.with_columns(rolling_expressions)\n\ndef join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Joins two dataframes by common columns and concatenates them vertically.\"\"\"\n    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n\ndef split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput:\n    \"\"\"Splits the data into features (X) and target (y), and scales the features.\"\"\"\n    X_train = train.drop(['date_id','target'])\n    y_train = train.get_column('target')\n    X_test = test.drop(['date_id','target'])\n    y_test = test.get_column('target')\n\n    scaler = StandardScaler()\n    \n    # Fit scaler only on the feature columns\n    X_train_scaled_np = scaler.fit_transform(X_train[features])\n    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n    \n    X_test_scaled_np = scaler.transform(X_test[features])\n    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n    \n    return DatasetOutput(\n        X_train = X_train,\n        y_train = y_train,\n        X_test = X_test,\n        y_test = y_test,\n        scaler = scaler\n    )\n\ndef convert_ret_to_signal(\n    ret_arr: np.ndarray,\n    params: RetToSignalParameters\n) -> np.ndarray:\n    \"\"\"Converts raw model predictions (expected returns) into a trading signal.\"\"\"\n    return np.clip(\n        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n    )\n\n# =================================================================\n# 4. DATA LOADING (MODIFIED)\n# =================================================================\nprint(\"Loading and preprocessing data...\")\ntrain_raw: pl.DataFrame = load_trainset().sort('date_id')\ntest_raw: pl.DataFrame = load_testset().sort('date_id')\n\n# --- FIX: Get common columns ---\n# This identifies all columns that exist in BOTH files\nglobal COMMON_COLS\nCOMMON_COLS = [col for col in train_raw.columns if col in test_raw.columns]\ntrain_raw = train_raw.select(COMMON_COLS)\ntest_raw = test_raw.select(COMMON_COLS)\n# Now both train_raw and test_raw have the *exact* same columns\n# (e.g., 'date_id', 'target', 'M1', 'V1', etc.)\n# This solves the ShapeError.\n\n# 1. Create row-wise features for train\ntrain_row_featured = create_example_dataset(train_raw)\n\n# 2. Create rolling features for train\ntrain = add_rolling_features(train_row_featured)\n\n# 3. Create raw history buffer for prediction API\nglobal HISTORY_BUFFER\nHISTORY_BUFFER = train_row_featured.clone()\n\n# 4. Create test set for local R^2 evaluation\ntest_row_featured = create_example_dataset(test_raw)\n# This concat will now work\ncombined_row_featured = pl.concat([train_row_featured, test_row_featured])\ncombined_rolled = add_rolling_features(combined_row_featured)\n\n# Split back to get the test set\ntest = combined_rolled.filter(pl.col('date_id').is_in(test_raw.get_column('date_id'))).sort('date_id')\n\n# 5. Define FEATURES list *after* all features are created\n# This now includes row-wise AND rolling features\nFEATURES: list[str] = [col for col in train.columns if col not in ['date_id', 'target']]\n\n# 6. Create scaled datasets\ndataset: DatasetOutput = split_dataset(train, test, features=FEATURES)\n\nX_train: pl.DataFrame = dataset.X_train\nX_test: pl.DataFrame = dataset.X_test\ny_train: pl.Series = dataset.y_train\ny_test: pl.Series = dataset.y_test\nscaler: StandardScaler = dataset.scaler\n\n# Convert to NumPy for sklearn\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\nX_test_np = X_test.to_numpy()\ny_test_np = y_test.to_numpy()\n\n# Define the time-series cross-validator\ntscv = TimeSeriesSplit(n_splits=N_SPLITS_TSCV)\n\nprint(\"Data processing complete.\")\n\n# =================================================================\n# 5. HYPERPARAMETER TUNING (KNN-ONLY)\n# =================================================================\nprint(\"\\n--- Starting KNN Hyperparameter Tuning ---\")\n\n# --- 1. Define Tuner ---\nknn_tuner = KNeighborsRegressor(\n    n_jobs=1      # Use 1 job to avoid CPU over-subscription\n)\n\n# --- 2. Define Parameter Grid ---\n# --- FIX: Widen the search range for n_neighbors ---\n# The model chose 200, which is high. We need to let it\n# search for even *more* neighbors to increase regularization.\nknn_param_grid = {\n    'n_neighbors': randint(150, 600), # Expanded range\n    'weights': ['uniform', 'distance'],\n    'metric': ['minkowski', 'manhattan']\n}\n\n# --- 3. Define Search Object (with silencing) ---\nknn_search = RandomizedSearchCV(\n    estimator=knn_tuner,\n    param_distributions=knn_param_grid,\n    n_iter=N_ITER_SEARCH,\n    cv=tscv,\n    scoring='r2',\n    n_jobs=1,\n    random_state=42,\n    verbose=0\n)\n\n# --- 4. Run Tuning with tqdm ---\nprint(\"Tuning KNN model...\")\nknn_search.fit(X_train_np, y_train_np)\n\n# --- 5. Get Best Model ---\nmodel_knn = knn_search.best_estimator_\nmodel_knn.set_params(n_jobs=1) # Ensure final model is also single-job\nprint(f\"Best KNN R^2 from tuning: {knn_search.best_score_:.4f}\")\nprint(f\"Best KNN params: {knn_search.best_params_}\")\n\n# =================================================================\n# 6. RUN THE OFFICIAL COMPETITION METRIC LOCALLY\n# =================================================================\n\n# --- 1. Define the score function ---\ndef score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n    \"\"\"\n    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n    \"\"\"\n\n    if not pd.api.types.is_numeric_dtype(submission['prediction']):\n        raise ParticipantVisibleError('Predictions must be numeric')\n\n    # This function modifies the solution df, so pass a copy\n    solution = solution.copy()\n    solution['position'] = submission['prediction']\n\n    if solution['position'].max() > MAX_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_INVESTMENT}')\n    if solution['position'].min() < MIN_INVESTMENT:\n        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_INVESTMENT}')\n\n    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n\n    # Calculate strategy's Sharpe ratio\n    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate']\n    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1\n    strategy_std = solution['strategy_returns'].std()\n\n    trading_days_per_yr = 252\n    if strategy_std == 0:\n        # Return 0 instead of crashing if std is zero\n        return 0.0 \n        \n    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr)\n    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)\n\n    # Calculate market return and volatility\n    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate']\n    market_excess_cumulative = (1 + market_excess_returns).prod()\n    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1\n    market_std = solution['forward_returns'].std()\n\n    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100)\n\n    if market_volatility == 0:\n        raise ParticipantVisibleError('Division by zero, market std is zero')\n\n    # Calculate the volatility penalty\n    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n    vol_penalty = 1 + excess_vol\n\n    # Calculate the return penalty\n    return_gap = max(\n        0,\n        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n    )\n    return_penalty = 1 + (return_gap**2) / 100\n\n    # Adjust the Sharpe ratio by the volatility and return penalty\n    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n    return min(float(adjusted_sharpe), 1_000_000)\n\nprint(\"\\n--- Calculating Local Score with Official Metric ---\")\n\n# --- 2. Create the 'solution' DataFrame ---\ntry:\n    solution_df = pd.read_csv(DATA_PATH / \"train.csv\")\n    # Keep only the columns the metric needs\n    solution_df = solution_df[['date_id', 'forward_returns', 'risk_free_rate']]\nexcept Exception as e:\n    print(f\"Error loading solution data: {e}\")\n    \n# --- 3. Create the 'submission' DataFrame ---\n# (UPDATED to use only the KNN model)\ntrain_date_ids = train.get_column('date_id').to_numpy()\n\n# Generate raw predictions for the training set\npreds_knn_train = model_knn.predict(X_train_np)\n\n# Convert raw returns to final 0-2 signals\nfinal_signals_train = convert_ret_to_signal(preds_knn_train, ret_signal_params)\n\n# Create the Pandas submission DataFrame\nsubmission_df = pd.DataFrame({\n    'date_id': train_date_ids,\n    'prediction': final_signals_train\n})\n\n# --- 4. Align the DataFrames ---\ncommon_ids = set(solution_df['date_id']).intersection(set(submission_df['date_id']))\n\nsolution_df = solution_df[\n    solution_df['date_id'].isin(common_ids)\n].sort_values('date_id').reset_index(drop=True)\n\nsubmission_df = submission_df[\n    submission_df['date_id'].isin(common_ids)\n].sort_values('date_id').reset_index(drop=True)\n\n# --- 5. Run the score function ---\nrow_id_col = 'date_id'\n\nif not solution_df.empty and not submission_df.empty:\n    try:\n        # Pass the prepared DataFrames to the score function\n        local_score = score(solution_df, submission_df, row_id_col)\n        \n        print(f\"\\n--- Your Local Adjusted Sharpe Ratio ---\")\n        print(f\"Score: {local_score:.5f}\")\n        \n    except ParticipantVisibleError as e:\n        print(f\"Error calculating score: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\nelse:\n    print(\"Could not calculate score: DataFrames were empty after alignment.\")\n\n\n# =================================================================\n# 7. LOCAL EVALUATION (R-Squared on Test Set)\n# =================================================================\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"\\n--- Evaluating Model Performance on Test Set ---\")\n\n# Get base model predictions\npreds_knn  = model_knn.predict(X_test_np)\n\ndef evaluate_model(name, y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"  {name}:\")\n    print(f\"    - MSE: {mse:.8f}\")\n    print(f\"    - R^2: {r2:.4f}\")\n\nprint(\"\\n--- Model Performance Metrics (R^2 > 0 is good) ---\")\nevaluate_model(\"KNN\", y_test_np, preds_knn)\n\n# --- View Final Signal Output vs Actual Signal ---\npredicted_signals = convert_ret_to_signal(preds_knn, ret_signal_params)\nactual_signals = convert_ret_to_signal(y_test_np, ret_signal_params)\n\nprint(\"\\n--- Signal Comparison (Test Set Examples) ---\")\nprint(\" Actual Return | Actual Signal | Predicted Return | Predicted Signal\")\nprint(\"---------------------------------------------------------------------\")\n\n# Loop only up to the number of available test samples, or 15, whichever is smaller\nn_examples = min(15, len(y_test_np)) \nfor i in range(n_examples):\n    print(f\"  {y_test_np[i]: >12.4f} |  {actual_signals[i]: >12.4f} |  {preds_knn[i]: >15.4f} |  {predicted_signals[i]: >16.4f}\")\n\n\n# =================================================================\n# 8. SUBMISSION API (MODIFIED)\n# =================================================================\n\ndef predict(test: pl.DataFrame) -> float:\n    \"\"\"\n    Predicts using ONLY the best-performing model (KNN).\n    Now includes stateful history for rolling features.\n    \"\"\"\n    global HISTORY_BUFFER\n    global COMMON_COLS # <-- Use the global list\n    \n    # 1. Rename target col for consistency\n    test = test.rename({'lagged_forward_returns':'target'})\n    \n    # 2. Create row-wise features for the new data\n    # Select *only* the columns we used for training\n    test_common = test.select(COMMON_COLS)\n    test_row_featured = create_example_dataset(test_common)\n    \n    # 3. Append new row-featured data to our history\n    # This concat will now work\n    HISTORY_BUFFER = pl.concat([HISTORY_BUFFER, test_row_featured])\n\n    # 4. Create rolling features on the *entire* history\n    df_rolled = add_rolling_features(HISTORY_BUFFER)\n    \n    # 5. Select the features for the *last* row(s)\n    # (test.height ensures we only predict on the new data)\n    X_test_pl = df_rolled.tail(test.height).select(FEATURES)\n    \n    if X_test_pl.is_empty():\n        return 1.0 # Return neutral signal\n\n    # 6. Scale the new data\n    X_test_numpy: np.ndarray = scaler.transform(X_test_pl)\n\n    # 7. Get prediction for the *very last* row\n    raw_pred: float = model_knn.predict(X_test_numpy)[-1]\n    \n    # 8. Convert to signal\n    return convert_ret_to_signal(raw_pred, ret_signal_params)\n\n# --- Run the inference server ---\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T13:09:40.410078Z","iopub.execute_input":"2025-10-27T13:09:40.410854Z","execution_failed":"2025-10-27T14:08:57.713Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nData processing complete.\n\n--- Starting KNN Hyperparameter Tuning ---\nTuning KNN model...\nBest KNN R^2 from tuning: 0.0008\nBest KNN params: {'metric': 'minkowski', 'n_neighbors': 339, 'weights': 'distance'}\n\n--- Calculating Local Score with Official Metric ---\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}